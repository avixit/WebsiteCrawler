# WebsiteCrawler
This is a website crawler based on DOM Parsing in Java. You may configure it in your eclipse as well.

Thanks to Jodd for the amazing set of Java micro frameworks, tools and utilities. (http://jodd.org/)

<b>Prerequisites : <br/></b>
1) Minimum Java 8 is required to run this. Please download latest version of Java from <a href="https://www.java.com/">here</a>

<b>Usage : <br/></b>
1) Configure the new project in Eclipse. You will need to add the dependency of "lib/jodd-all-3.8.0.jar" file in your build path. <br/>
2) Compile and run the "WebsiteURLCrawler.java/class" file. By default it will list all the url's of most popular online coding website <a href="https://www.hackerearth.com">hackerrank</a> <br/>
3) Currently it will filter following type of data : External URL, PDF, JPG/JPEG and PNG <br/>
4) Should you need any help in using/configuring or you have any suggestions or improvements or want to contribute, please shoot me an email (avixit.aparnathi@gmail.com) with all the details. I shall definitely get back to you <br/>

<b>Future Scope : <br/></b>
1) Add cookie support to fetch all the url's after login in a website </br>
2) Add "Save to a file" support to save all the details in popular file formats (i.e Text, HTML) </br>
3) Add dynamic filtering (i.e If you want only images or PDF or any specific type of files from a url) </br>

Happy Crawling !